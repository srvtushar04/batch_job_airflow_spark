version: "3.9"

services:
  airflow-db:
    image: postgres:16
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 10s
      timeout: 5s
      retries: 5
    volumes:
      - airflow_db:/var/lib/postgresql/data

  minio:
    image: minio/minio:RELEASE.2025-02-07T00-00-00Z
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ACCESS_KEY}
      MINIO_ROOT_PASSWORD: ${MINIO_SECRET_KEY}
    ports:
      - "9000:9000"   # S3 endpoint
      - "9001:9001"   # Console
    volumes:
      - minio_data:/data

  create-buckets:
    image: minio/mc:latest
    depends_on:
      - minio
    entrypoint: >
      /bin/sh -c "
        mc alias set local http://minio:9000 ${MINIO_ACCESS_KEY} ${MINIO_SECRET_KEY} &&
        mc mb -p local/raw-zone || true &&
        mc mb -p local/processed-zone || true &&
        mc mb -p local/logs || true &&
        exit 0
      "

  airflow:
    image: apache/airflow:2.9.3-python3.10
    depends_on:
      - airflow-db
      - minio
      - create-buckets
    env_file:
      - .env
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "True"
      # Spark & S3 configs injected into tasks
      SPARK_S3_ENDPOINT: http://minio:9000
      SPARK_S3_ACCESS_KEY: ${MINIO_ACCESS_KEY}
      SPARK_S3_SECRET_KEY: ${MINIO_SECRET_KEY}
      RAW_BUCKET: raw-zone
      PROCESSED_BUCKET: processed-zone
      RAW_PREFIX: products_api/dt={{ ds }}
      PROCESSED_PREFIX: products_clean/dt={{ ds }}
      # Snowflake creds (fill in .env)
      SNOWFLAKE_USER: ${SNOWFLAKE_USER}
      SNOWFLAKE_PASSWORD: ${SNOWFLAKE_PASSWORD}
      SNOWFLAKE_ACCOUNT: ${SNOWFLAKE_ACCOUNT}
      SNOWFLAKE_WAREHOUSE: ${SNOWFLAKE_WAREHOUSE}
      SNOWFLAKE_DATABASE: ${SNOWFLAKE_DATABASE}
      SNOWFLAKE_SCHEMA: ${SNOWFLAKE_SCHEMA}
    volumes:
      - ./dags:/opt/airflow/dags
      - ./scripts:/opt/airflow/scripts
      - ./configs:/opt/airflow/configs
      - ./great_expectations:/opt/airflow/great_expectations
      - ./requirements.txt:/requirements.txt
    command: >
      bash -c "
        pip install --no-cache-dir -r /requirements.txt &&
        airflow db init &&
        airflow users create --role Admin --username admin --password admin --firstname a --lastname b --email admin@example.com || true &&
        airflow webserver & airflow scheduler
      "
    ports:
      - "8080:8080"

volumes:
  airflow_db:
  minio_data:
